{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### <p> Samuel Wolfe <br> June 3rd, 2023 <br> MSBA 206 <br> DMBA Chapter 2 </p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.1\n",
    "<p>a. Supervised <br>\n",
    "b. Unsupervised <br>\n",
    "c. Supervised <br>\n",
    "d. Unsupervised <br>\n",
    "e. Supervised <br>\n",
    "f. Supervised <br>\n",
    "g. Supervised <br>\n",
    "h. Unsupervised</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.2\n",
    "<p>The difference between roles assumed by validation partition and the test partition is that validation assesses the predictive performance of each model so that we can compare models and choose the best one while testing assess the performance of the chosen model with new data. This minor difference in testing between stages prevents over fitting during the training phase.\n",
    "</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.3"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>No, the sample database of credit applicants in table 2.16 are not random. If they were truly random they would not have an OBS value divisible by eight. This indicates to me that this sample is not randomly generated.</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.4\n",
    "<p>For the given data of Table 2.17 I would start by doing regression analysis on each variable to determine their impact on Personal Loan. Without this step it is impossible to estimate the impact of any single element in the prediction model.</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.5\n",
    "<p>When a model is perfectly fitted to the data, it means it is not correctly associating the noise found within all data sets correctly for this situation and will fail to accurately predict during your test partition.</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.6\n",
    "<p>\"Refund issued\" is not an appropriate variable to include because it comes from elements of data not actually found in the future data sets. If the company wanted to train their models accurately they would exclude purchase data in their training data sets. Or they should look into data sources that include this type of data if \"refund issued\" indeed shows a much better predictor of purchasers or nonpurchasers.</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.7\n",
    "<p>Given 1000 records and 50 variables with a 5% chance of having missing values, means about 77 records would be lost.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "76.94497527671315"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "records = 1000\n",
    "variables = 50\n",
    "chance = 0.05\n",
    "((1-chance)**variables)*records"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.8 Normalizing the data from Table 2.18."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>age</th>\n",
       "      <th>Income ($)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>-1.313253</td>\n",
       "      <td>-0.790027</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.756790</td>\n",
       "      <td>0.911977</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1.357770</td>\n",
       "      <td>0.005302</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>-0.845824</td>\n",
       "      <td>1.484614</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>-0.244844</td>\n",
       "      <td>-0.949093</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>5</th>\n",
       "      <td>0.289361</td>\n",
       "      <td>-0.662774</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        age  Income ($)\n",
       "0 -1.313253   -0.790027\n",
       "1  0.756790    0.911977\n",
       "2  1.357770    0.005302\n",
       "3 -0.845824    1.484614\n",
       "4 -0.244844   -0.949093\n",
       "5  0.289361   -0.662774"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "tbl218 = [[25, 49000],[56,156000],[65,99000],[32,192000],[41,39000],[49,57000]]\n",
    "pdtbl218 = pd.DataFrame(tbl218, columns=[\"age\", \"Income ($)\"])\n",
    "normaltbl218 = (pdtbl218 - pdtbl218.mean())/pdtbl218.std()\n",
    "normaltbl218"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.9\n",
    "<p>Normalizing the data changes which records are the farthest from each other in terms of Euclidean distance. Because of the way Euclidean distance works if values are to large in comparison to other values, it gives disproportional weight to values. After normalization our values will be different, but more accurate.</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.10\n",
    "<p>Given when training models you first go validation then test, given the parlance of the textbook, then I would assume that Model A is better for deployment, as Model B appears to be over fitted to the validation data.</p>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### 2.11\n",
    "<p>2.11.a. To convert the attributes I first figure out how many unique values there are for each, isolating them into a fresh data frame. Then I run each new data frame in a for loop use the intrinsic x of the for loop to iterate over every single entry in the main data frame for that specific column. There might be a more efficient way to do this but this is what I came up with.</p>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['Blue' 'Silver' 'Black' 'White' 'Grey' 'Red' 'Green' 'Yellow' 'Violet'\n",
      " 'Beige']\n",
      "['Diesel' 'Petrol' 'CNG']\n",
      "[0 1 2 3 4 5 6 7 8 9]\n",
      "[0 1 2]\n",
      "Whole      :  (1436, 39)\n",
      "Training   :  (718, 39)\n",
      "Validation :  (431, 39)\n",
      "Test       :  (287, 39)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "fileLocation = r'E:\\Aliit\\School\\MSBA\\206\\dmba-datasets\\ToyotaCorolla.csv'\n",
    "dfToyotaCorolla = pd.read_csv(fileLocation)\n",
    "colors = pd.DataFrame(dfToyotaCorolla['Color'].unique())\n",
    "fuels = pd.DataFrame(dfToyotaCorolla['Fuel_Type'].unique())\n",
    "print(dfToyotaCorolla['Color'].unique())\n",
    "print(dfToyotaCorolla['Fuel_Type'].unique())\n",
    "for x in colors[0]:\n",
    "    dfToyotaCorolla['Color'].mask(dfToyotaCorolla['Color'] == x, colors[colors[0]==x].index.values, inplace=True)\n",
    "for x in fuels[0]:\n",
    "    dfToyotaCorolla['Fuel_Type'].mask(dfToyotaCorolla['Fuel_Type'] == x, fuels[fuels[0]==x].index.values, inplace=True)\n",
    "print(dfToyotaCorolla['Color'].unique())\n",
    "print(dfToyotaCorolla['Fuel_Type'].unique())\n",
    "dfTraining = dfToyotaCorolla.sample(frac=0.5, random_state=1)\n",
    "dfValidation = dfToyotaCorolla.drop(dfTraining.index).sample(frac=0.6, random_state=1)\n",
    "dfTesting = dfToyotaCorolla.drop(dfTraining.index).drop(dfValidation.index)\n",
    "print('Whole      : ', dfToyotaCorolla.shape)\n",
    "print('Training   : ', dfTraining.shape)\n",
    "print('Validation : ', dfValidation.shape)\n",
    "print('Test       : ', dfTesting.shape)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
